# GSoC 2022 | Python Software Foundation (Activeloop)

## Introduction

This summer, I got accepted as a contributor to the Python Software Foundation (Activeloop) in the 2022 Google Summer of Code. Activeloop's open-source open source package named [Hub](https://github.com/activeloopai/Hub) lets you store (even petabyte-scale) datasets on the cloud, and lets you seamlessly integrate it with your ML applications. The goal is to develop a set of data-centric APIs for a machine learning library that can optimize datasets with minimal parameters. I had about a month to come up with a technical solution. In this one month, I had to synthesize the requirements, research a large number of data-centric strategies (e.g., data augmentation, noise cleaning, data selection, self-supervision), review academic papers, and develop end-to-end pipelines for machine learning experiments to benchmark the performance of data-centric strategies for various computer vision tasks (e.g., classification). Taking into account all the possible strategies, there exists a huge number of options. Each of these specific strategies has hundreds of hyperparameters, and the way they are structured impacts the downstream model.

This is an ambiguous task as you need to be capable of understanding the high-level business problem down to the lines of code. If I were a user and this hasn’t been implemented yet, how would I go about doing it myself? Asking this question helped to understand the end-user and uncover that the process is highly iterative. I communicated with mentors, built a high-level overview of the process, then broke it down into subproblems and separately optimized each component. This has helped me to focus on exploring and evaluating each strategy one at a time without losing attention to the ambiguous high-level problem I'm trying to solve. As the project has a time constraint, I had to challenge myself with a question: *"How can I implement a strategy that will yield the most impact for the end-users, given the tight timeline?"*

## Research Phase
Early on during the research phase, the mentors challenged me on what could be done  before proceeding with some advanced strategies, such as data augmentation or data selection. Are there any fundamental flaws in the data we have at hand? What could be done in a subset of ML problems, such as supervised learning? Today, most practical machine learning models utilize supervised learning. For supervised learning to work, you need a labeled set of data that the model can learn from to make correct decisions. Data labeling typically starts by asking humans to make judgments about a given piece of unlabeled data. For example, labelers may be asked to tag all the images in a dataset where an image contains a *car*. The machine learning model uses human-provided labels to learn the underlying patterns in a process called *model training*. The result is a trained model that can be used to make predictions on new data. Supervised learning is the [dominant ML system at Google](https://developers.google.com/machine-learning/intro-to-ml/supervised). Because supervised learning's tasks are well-defined, like identifying a class of an image, it has more potential use cases than unsupervised learning. 

In machine learning, a properly labeled dataset that you use as the objective standard to train and assess a given model is often called *ground truth*. The accuracy of your trained model will depend on the accuracy of your ground truth, so spending the time and resources to ensure highly accurate data labeling is essential. 
If you’ve ever used datasets like CIFAR, MNIST, ImageNet, or IMDB, you likely assumed the class labels are correct. Supervised ML often assumes that the labels we train our model on are correct, but [recent studies](https://www.technologyreview.com/2021/04/01/1021619/ai-data-errors-warp-machine-learning-progress/) have discovered that even highly-curated ML benchmark datasets are full of [label errors](https://labelerrors.com/). What's more, the [Northcutt’s NeurIPS 2021](https://arxiv.org/abs/2103.14749) work on analyzing errors in datasets found out that hundreds of samples across popular datasets where an agreement could not be reached on true ground truth despite looking at collating outcomes from labelers. Furthermore, the labels in datasets from real-world applications can be of [far lower quality](https://go.cloudfactory.com/hubfs/02-Contents/3-Reports/Crowd-vs-Managed-Team-Hivemind-Study.pdf). There are several factors that come into play that lead to error in the dataset, such as a human error made while annotating the examples. These days, it is increasingly be the training data, not the models, or infrastructure, that decides whether a machine learning will be a success or failure. However, it seems problematic to train our ML models to predict fundamentally flawed labels. This becomes especially problematic when these errors reach test sets, the subsets of datasets used to validate the trained model. 
Even worse, we might train and evaluate these models with flawed labels and deploy the resulting models at scale. 

## Hub + Cleanlab
Hub community has uploaded a variety of popular machine learning datasets like [CIFAR-10](https://docs.activeloop.ai/datasets/cifar-10-dataset), MNIST or Fashion-MNIST and [ImageNet](https://docs.activeloop.ai/datasets/imagenet-dataset/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme). Without any need to download, these datasets can be accessed and streamed with Hub with one line of code. This enables you to explore the datasets and train models without needing to download machine learning datasets regardless of their size. However, most of these datasets contain [label errors](https://labelerrors.com/). What can we do about this? To tackle this problem, the Northcutt’s group of researchers co-founded [Cleanlab](https://cleanlab.ai/), a tool that allows to automatically find and fix label errors in ML datasets. Under the hood, it uses [Confident Learning](https://arxiv.org/abs/1911.00068) (CL) algorithm to detect label errors. 

Nevertheless, Hub dataset is a [specific format of a dataset](https://docs.activeloop.ai/how-hub-works/data-layout) which uses a columnar storage architecture, and the columns are referred to as tensors. It is not trivial on how to set up the workflow for finding label errors, as Cleanlab does not support Hub datasets as of now. Therefore, users are required to make their models and data format compatitable and then run a training pipeline that would allow them to utilize Cleanlab to find label issues. How can we abstract this complexity away and provide users with a clean and intuitive interface to find label errors? Furthermore, how can we use Hub's powerful [data visualization](https://docs.activeloop.ai/dataset-visualization) tools to enable users to visualize these insights and make informed decisions on what to do with noisy data?

## Experiments
We've talked about how label errors can be be detrimental to the success of an ML project. In theory, this assumption holds a strong argument. However, we wanted to prove it by running a few experiments before we introduced this feature to our users. 

By communicating with my mentors, we set up a few research questions that we were looking to answer:

> *What is the impact of label errors on a model performance? Can we implement an experiment that will enable us quantify the impact? How do we ensure that our experiments are unbiased and reproducible?*

> *Assuming that the label errors could be found, what would be the next steps? Can we find a way to automatically prune noisy examples from a dataset? Does it make sense to leave some of the examples, but correct their labels?*

