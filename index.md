# GSoC 2022 | Python Software Foundation (Activeloop)

This is a summary of the work I did for Activeloop's open-source open source package named [Hub](https://github.com/activeloopai/Hub) under the Python Software Foundation organization as part of Google Summer of Code 2022.

## Project Details
- Title: [Automated Dataset Tuning](https://summerofcode.withgoogle.com/programs/2022/projects/o6DWVntH)
- Organization: [Python Software Foundation](https://www.python.org/psf/)
- Repository: [Activeloop Hub](https://github.com/activeloopai/Hub)
- Mentors: [Davit Buniatyan](https://github.com/davidbuniat), [Fariz Rahman](https://github.com/farizrahman4u), [Ivo Stranic](https://github.com/istranic), [Mikayel Harutyunyan](https://github.com/mikayelh)


## Introduction

This summer, I got accepted as a contributor to the Python Software Foundation (Activeloop) in the 2022 Google Summer of Code. Activeloop's open-source open source package named [Hub](https://github.com/activeloopai/Hub) lets you store (even petabyte-scale) datasets on the cloud, and lets you seamlessly integrate it with your ML applications. The goal is to develop a set of data-centric APIs for a machine learning library that can optimize datasets with minimal parameters. I had about a month to come up with a technical solution. In this one month, I had to synthesize the requirements, research a large number of data-centric strategies (e.g., data augmentation, noise cleaning, data selection, self-supervision), review academic papers, and develop end-to-end pipelines for machine learning experiments to benchmark the performance of data-centric strategies for various computer vision tasks (e.g., classification). Taking into account all the possible strategies, there exists a huge number of options. Each of these specific strategies has hundreds of hyperparameters, and the way they are structured impacts the downstream model.

This is an ambiguous task as you need to be capable of understanding the high-level business problem down to the lines of code. If I were a user and this hasn’t been implemented yet, how would I go about doing it myself? Asking this question helped to understand the end-user and uncover that the process is highly iterative. I communicated with mentors, built a high-level overview of the process, then broke it down into subproblems and separately optimized each component. This has helped me to focus on exploring and evaluating each strategy one at a time without losing attention to the ambiguous high-level problem I'm trying to solve. As the project has a time constraint, I had to challenge myself with a question: *"How can I implement a strategy that will yield the most impact for the end-users, given the tight timeline?"*

## Research Phase
Early on during the research phase, the mentors challenged me on what could be done  before proceeding with some advanced strategies, such as data augmentation or data selection. Are there any fundamental flaws in the data we have at hand? What could be done in a subset of ML problems, such as supervised learning? Today, most practical machine learning models utilize supervised learning. For supervised learning to work, you need a labeled set of data that the model can learn from to make correct decisions. Data labeling typically starts by asking humans to make judgments about a given piece of unlabeled data. For example, labelers may be asked to tag all the images in a dataset where an image contains a *car*. The machine learning model uses human-provided labels to learn the underlying patterns in a process called *model training*. The result is a trained model that can be used to make predictions on new data. Supervised learning is the [dominant ML system at Google](https://developers.google.com/machine-learning/intro-to-ml/supervised). Because supervised learning's tasks are well-defined, like identifying a class of an image, it has more potential use cases than unsupervised learning. 

In machine learning, a properly labeled dataset that you use as the objective standard to train and assess a given model is often called *ground truth*. The accuracy of your trained model will depend on the accuracy of your ground truth, so spending the time and resources to ensure highly accurate data labeling is essential. 
If you’ve ever used datasets like CIFAR, MNIST, ImageNet, or IMDB, you likely assumed the class labels are correct. Supervised ML often assumes that the labels we train our model on are correct, but [recent studies](https://www.technologyreview.com/2021/04/01/1021619/ai-data-errors-warp-machine-learning-progress/) have discovered that even highly-curated ML benchmark datasets are full of [label errors](https://labelerrors.com/). What's more, the [Northcutt’s NeurIPS 2021](https://arxiv.org/abs/2103.14749) work on analyzing errors in datasets found out that hundreds of samples across popular datasets where an agreement could not be reached on true ground truth despite looking at collating outcomes from labelers. Furthermore, the labels in datasets from real-world applications can be of [far lower quality](https://go.cloudfactory.com/hubfs/02-Contents/3-Reports/Crowd-vs-Managed-Team-Hivemind-Study.pdf). There are several factors that come into play that lead to error in the dataset, such as a human error made while annotating the examples. These days, it is increasingly be the training data, not the models, or infrastructure, that decides whether a machine learning will be a success or failure. However, it seems problematic to train our ML models to predict fundamentally flawed labels. This becomes especially problematic when these errors reach test sets, the subsets of datasets used to validate the trained model. 
Even worse, we might train and evaluate these models with flawed labels and deploy the resulting models at scale. 

## Hub + Cleanlab
Hub community has uploaded a variety of popular machine learning datasets like [CIFAR-10](https://docs.activeloop.ai/datasets/cifar-10-dataset), MNIST or Fashion-MNIST and [ImageNet](https://docs.activeloop.ai/datasets/imagenet-dataset/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme). Without any need to download, these datasets can be accessed and streamed with Hub with one line of code. This enables you to explore the datasets and train models without needing to download machine learning datasets regardless of their size. However, most of these datasets contain [label errors](https://labelerrors.com/). What can we do about this? To tackle this problem, the Northcutt’s group of researchers co-founded [Cleanlab](https://cleanlab.ai/), a tool that allows to automatically find and fix label errors in ML datasets. Under the hood, it uses [Confident Learning](https://arxiv.org/abs/1911.00068) (CL) algorithm to detect label errors. 

Nevertheless, Hub dataset is a [specific format of a dataset](https://docs.activeloop.ai/how-hub-works/data-layout) which uses a columnar storage architecture, and the columns are referred to as tensors. It is not trivial on how to set up the workflow for finding label errors, as Cleanlab does not support Hub datasets as of now. Therefore, users are required to make their models and data format compatitable and then run a training pipeline that would allow them to utilize Cleanlab to find label issues. How can we abstract this complexity away and provide users with a clean and intuitive interface to find label errors? Furthermore, how can we use Hub's powerful [data visualization](https://docs.activeloop.ai/dataset-visualization) tools to enable users to visualize these insights and make informed decisions on what to do with noisy data?

## Experiments
We've talked about how label errors can be be detrimental to the success of an ML project. In theory, this assumption holds a strong argument. However, we wanted to prove it by running a few experiments before we introduced this feature to our users. 

By communicating with my mentors, we set up a few research questions that we were looking to answer:

> ***Research Question 1**: What is the impact of label errors on a model performance? Can we implement an experiment that will enable us quantify the impact? How do we ensure that our experiments are unbiased and reproducible?*
> ***Research Question 2**: Assuming that the label errors could be found, what would be the next steps to fix the errors? Can we find a way to automatically prune or relabel noisy examples from a dataset? Does it make sense to leave some of the examples, but correct their labels?*

### Label Errors Impact on Model Performance 

Let's try to break down the first research question. To measure the impact of label errors, we'll need to find a metric to optimize for. Accuracy is one metric for evaluating supervised models, which is the fraction of predictions that a model got right. Now, to quantify the impact of label errors, we'll need a way to benchmark a model trained on clean data and a model trained on noisy data. By looking at the accuracy of the predictions made by a model trained on noisy data and on clean data, we can compare the approaches against each other. Let's suppose we can compare this metric on some dataset. However, we don't necessarily know the ground truth, as there could already be errors in the dataset. We can only be certain if we introduce these label errors ourselves, assuming that we do it on a dataset that has a relatively low rate of label errors in the first place. To overcome this, we can artificially introduce some noise to a dataset that we assume has a low rate of label errors. We can introduce some random noise to the training set by randomly flipping the labels. 

For the experiment, let's use [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and assume that it has a low rate of errors. Then, we gradually introduce **5%** of the noise at each step, comparing the performance of `Baseline` and `Cleanlab` in parallel. In this case, `Baseline` is a model trained on noisy data, while `Cleanlab` is a model trained on a clean data. Here’s an example: *if we have **60,000** samples in the dataset, at **10%** of the noise, we would randomly flip **6,000** labels*. The `Baseline` would then be trained on all **60,000** samples, but the `Cleanlab` would be trained only on the examples that weren’t classified as erroneous. 

![enter image description here](https://github.com/lowlypalace/gsoc-2022/raw/main/accuracy_plot.png)

It looks that pruning the noisy samples works well, as Cleanlab seems to accurately detect the erroneous labels that were introduced at each noise level. We can also see that the accuracy stays around **85%** with Cleanlab across a range of noise levels, while with the random noise (without removing any samples) it drops at much higher rate to a low of **65%** at the highest noise level.


![enter image description here](https://github.com/lowlypalace/gsoc-2022/raw/main/samples_plot.png)
Looking at the graph above, we can see that Cleanlab on average prunes more labels errors than we introduced. We can argue that there might have been some initial label errors in the dataset, before we introduced them ourselves. However, it might be that Cleanlab is overestimating. Nevertheless, it seems to be able to systematically pick up the newly introduced noisy labels and identify them as erroneous. As we mentioned, the foundation CL depends on is that label noise is class-conditional, depending only on the latent true class, not the data. In the real-world noisy data, the mislabelling between different classes would have a stronger statistical dependence, so we can say that this example was even a bit more difficult for Cleanlab as we were swapping the labels at random.

### Fixing Label Errors
Next, let's try to break down second research question. Now that we have found out that we can systematically detect label errors, what would be the next steps? As a user, I want to be able to get the insights and do something to my noisy data. If a sample is mislabaled but has a meaningful data, do I leave out these examples and relabel them to the correct labels? If a sample is simply noise (e.g. a corrupted image), do I remove this example? Furthermore, is there an automatic way to set this up?

Let's try to break it down further. Again, we'll need a way to benchmark the approaches and compare them against each other. What if we ask ourselves a simpler question: *now that we have label errors, what if we remove the examples that are the noisiest (i.e., has little or no meaningful data) and attempt to fix or relabel the examples that are less noisy.* As Cleanlab provides us with quality scores for each example, we can try to prune the labels with lowest quality scores, and relabel the rest of the labels. Assumably, after training a model on clean data, we should be able to get the updated predictions for each label, which we can use to relabel the less noisy labels. 

As a next step, we try to set a threshold which would tell us the ratio of examples that we will remove to the examples that we will relabel. With a threshold of 10%, we will prune first top 10% labels with lowest quality and relabel the rest of examples. We can then see how the accuracy across various noise levels compares. Here, we tried to experiment with different threshold values for pruning and relabelling the images (remove 70% of the images with lowest label quality, but leave and relabel the rest). We started with a threshold of 0% (e.g. relabel all labels to the guessed labels) and then gradually increased the threshold value with a 10% step till we reached 100% prune level (remove all labels that were found to be erroneous). 

![enter image description here](https://github.com/lowlypalace/gsoc-2022/raw/main/accuracy_threshold_plot.png)

On the graph, we plotted the accuracy of the models trained with training sets that were fixed with different threshold values. For example, `100% Prune / 0% Relabel` indicates the accuracy of the model when all erroneous samples and their labels were deleted, while `0% Prune / 100% Relabel` shows the accuracy of the model when all of the samples were left but relabelled.

Looking at the graph, we can say that Cleanlab definitely does a great job at identifying labels, but not necessarily at relabeling them automatically. As soon as we increase the number of labels that we’d like to relabel, the accuracy starts to go down in linear way. The training set with **100%** of pruning got the highest accuracy, while the training set with all labels relabelled got the worst accuracy. Sometimes, the confident learning approach can get it wrong too, like confusing a correctly labeled image. Therefore, it is best to go through the examples in a dataset and make a decision on whether remove or relabel an example. With these insights, we can know that it makes sense to provide users with functionality to automatically prune examples that are erroneous.


## Experience
I had an ambigious high-level problem that I was trying to solve, and I was fortunate that the mentors have given me a lot of freedom to solve this problem. It’s not something I can take for granted as I had a high responsibility to my mentors, but It was really rewarding to own the whole technical process from big idea to shipping out the solution. During my GSoC, I found myself drawing on much more than just my experience in software engineering. For example, I utilized my experience in academic research, presentation skills and writing to successfully execute the project. Additionally, I improved my leadership and communication skills by co-leading community efforts, where I welcomed new open source contributors to Hub, assigned them tasks and helping them out to get started. 

I think that the reason I succeeded in this project is that even though the codebase is immense and unfamiliar to me, I learned to ask the right questions to understand the scope of a problem. Even if I’m unfamiliar with the technology itself, I can ask strategic questions to get enough understanding to work towards a solution. I also always come back to thinking about how a user might experience a feature or what else they might need or want. This has helped me to stay focused on the problem I'm solving, even though my high-level . It’s also important to not be afraid when facing a new problem. In my day to day, I was constantly working on things that are new to me or that I’ve never done before. This is just one feature I developed within a large codebase, but it shows how I could start with high-level goal, carefully consider the technical implications and design a  solution.
